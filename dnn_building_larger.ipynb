{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from adabelief_pytorch import AdaBelief\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Creating Datasets and DataLoaders</h2>\n",
    "\n",
    "We will create two different dataset classes: one for our training data, and one for our testing data. Once these are done,we will also create a two dataloaders that will be used to create our training and testing data. We will use the dataloaders to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset and dataloader.\n",
    "class PitchDatasetTrain(Dataset):\n",
    "    def __init__(self):\n",
    "        # Read in data\n",
    "        df = pd.read_parquet('training_data.parquet')\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        x_data = df.drop(columns=['pitch_type'])\n",
    "        self.X = torch.tensor(x_data.to_numpy().astype(np.float32))\n",
    "        self.y = torch.tensor(df['pitch_type'].values, dtype=torch.long)\n",
    "        # self.y = self.y.reshape(-1, 1)\n",
    "        self.n_samples = df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "    \n",
    "class PitchDatasetTest(Dataset):\n",
    "    def __init__(self):\n",
    "        # Read in data\n",
    "        df = pd.read_parquet('test_data.parquet')\n",
    "\n",
    "        # Convert data to PyTorch tensors\n",
    "        x_data = df.drop(columns=['pitch_type'])\n",
    "        self.X = torch.tensor(x_data.to_numpy().astype(np.float32))\n",
    "        self.y = torch.tensor(df['pitch_type'].values, dtype=torch.long)\n",
    "        self.n_samples = df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "# Create the dataset and dataloader.\n",
    "train_data = PitchDatasetTrain()\n",
    "test_data = PitchDatasetTrain()\n",
    "batch_size, num_workers = 32, 2\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Helper Functions</h3>\n",
    "\n",
    "These are the functions we will use in our model and/or training loop. So far we only have a function to calculate the accuracy of our model, and our custom activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Checker Function\n",
    "def calculate_accuracy(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return correct / total\n",
    "\n",
    "# Define new activation function PenalizedTanH\n",
    "class PenalizedTanH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PenalizedTanH, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.where(x > 0, torch.tanh(x), 0.25*torch.tanh(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The Model</h3>\n",
    "\n",
    "This has our deep learning model. It is bigger than the original. We now use use the <code>AdaBelief</code>\n",
    "optimizer and the <code>CrossEntropyLoss</code> loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]:   1%|          | 860/82368.0 [03:21<5:18:08,  4.27it/s, acc=0.000171, loss=2]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4d/s5cpw8t92xbf0b9gw8j7ny2m0000gn/T/ipykernel_48464/4002923211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mloss_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcurr_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/acme1/lib/python3.7/site-packages/adabelief_pytorch/AdaBelief.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mN_sma\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                         \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build the DNN\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# First, define the hyperparameters\n",
    "input_size = 31  # Input size (e.g., number of features)\n",
    "hidden_size = 64  # Size of the hidden layer(s)\n",
    "output_size = 19  # Output size (e.g., number of classes)\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define the neural network architecture\n",
    "class PitchDNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(PitchDNN, self).__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(input_size, hidden_size),\n",
    "                                 PenalizedTanH(),\n",
    "                                 nn.Linear(hidden_size, hidden_size*2),\n",
    "                                 PenalizedTanH(),\n",
    "                                 nn.Linear(hidden_size*2, hidden_size*3),\n",
    "                                 PenalizedTanH(),\n",
    "                                 nn.Linear(hidden_size*3, hidden_size*4),\n",
    "                                 PenalizedTanH(),\n",
    "                                 nn.Linear(hidden_size*4, hidden_size*4),\n",
    "                                 PenalizedTanH(),\n",
    "                                 nn.Linear(hidden_size*4, hidden_size*3),\n",
    "                                 PenalizedTanH(),\n",
    "                                 nn.Linear(hidden_size*3, hidden_size*2),\n",
    "                                 PenalizedTanH(),\n",
    "                                 nn.Linear(hidden_size*2, hidden_size),\n",
    "                                 PenalizedTanH(),\n",
    "                                 nn.Linear(hidden_size, output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = PitchDNN(input_size, hidden_size, output_size)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdaBelief(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define variables for training loop.\n",
    "num_epochs = 10\n",
    "total_samples = len(train_data)\n",
    "n_iterations = np.ceil(total_samples/batch_size)\n",
    "\n",
    "# Define variables for storing our loss and accuracy.\n",
    "loss_vals, loss_occ = [], []\n",
    "acc_vals, acc_occ = [], []\n",
    "\n",
    "# Run training loop.\n",
    "for epoch in range(num_epochs):\n",
    "    # Create our tqdm progress bar.\n",
    "    tqdm_data_loader = tqdm(train_dataloader, total=n_iterations, desc=f'Epoch [{epoch + 1}/{num_epochs}]', dynamic_ncols=True)\n",
    "\n",
    "    # Check the model's accuracy (do this for every epoch).\n",
    "    acc_vals.append(calculate_accuracy(model, test_dataloader))\n",
    "\n",
    "    # Create a list to hold the current epochs loss values (to average for the tqdm bar).\n",
    "    curr_loss = []\n",
    "\n",
    "    # Train the model.\n",
    "    for i, (inputs, labels) in enumerate(tqdm_data_loader):\n",
    "        optimizer.zero_grad()  # Clear gradients from previous iteration\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update model parameters\n",
    "        loss_vals.append(loss.item())\n",
    "        curr_loss.append(loss.item())\n",
    "\n",
    "        # Update our tqdm loop so we can see what is happening.\n",
    "        tqdm_data_loader.set_postfix(loss=np.mean(curr_loss), acc=acc_vals[-1])\n",
    "    \n",
    "\n",
    "# After training, you can save the model\n",
    "torch.save({'model': model.state_dict(),\n",
    "            'optim': optimizer.state_dict()}, 'pitch_dnn_L1.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Plots</h3>\n",
    "\n",
    "This is a simple plot of our loss and accuracy. We will use this to see how our model is doing. As you can see, it hits the desired accuracy pretty quickly, and then just stays there. The loss is a little weird, but it is not too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the outputs of our model (the training and the accurary)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8,6), dpi=100)\n",
    "\n",
    "# Plot the loss\n",
    "ax[0].plot(np.linspace(0, num_epochs, len(loss_vals)), loss_vals)\n",
    "ax[0].set_xlabel('Time Step')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_title('Loss of the Pitch DNN')\n",
    "\n",
    "# Plot the accuracy\n",
    "ax[1].plot(np.linspace(0, num_epochs, len(acc_vals)), acc_vals)\n",
    "ax[1].set_xlabel('Time Step')\n",
    "ax[1].set_ylabel('Accurary (%)')\n",
    "ax[1].set_title('Accuracy of the Pitch DNN')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acme1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
